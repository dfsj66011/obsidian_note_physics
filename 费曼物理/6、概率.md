
> [!NOTE]
> 我们这个世界的真正逻辑寓于概率的计算之中。                                              ——J.C.麦克斯韦
> 

## 6–1、机会和可能性

“机会”是⽇常⽣活中通常使⽤的⼀个词汇。⽆线电在播送明天的天⽓预报时可能会说：“明天下⾬的机会是 60%。”你也许会说：“我能活上 100 岁的机会是不⼤的。”科学家也使⽤机会这个词。⼀个地震学家可能会对这样的问题感兴趣：“明年在南加利福尼亚州发⽣某⼀级地震的机会有多⼤？”⼀个物理学家也许会提出这样的问题：“在下⼀个 $10\,\text{s}$ 内，某⼀特定盖⾰计数器将记录到 $20$ 个计数的机会是多少？”⼀个政治家或国务活动家可能对下列问题感兴趣：“下⼀个 $10$ 年内发⽣核战争的机会是多少？”同样，你也许会对从这⼀章中将学到⼀些东西的机会发⽣兴趣。

所谓 *机会* 指的是某种类似于猜测的事。为什么我们要猜测呢？希望作出判断⽽只掌握不完全的信息或不确定的知识时，我们就要进⾏猜测。我们要对这是些什么东西或者可能会发⽣什么事情进⾏猜测。由于必须作出决定，我们常常要进⾏猜测。⽐如说，明天我是否要带上⾬⾐？我应设计⼀座能够防御哪种程度地震的新⼤厦？我是否要为⾃⼰建造⼀个放射性微粒掩蔽所？我是否要在国际谈判中改变⾃⼰的⽴场？我今天是否要去上课？

有时我们所以要进⾏猜测，是因为我们想⽤⾃⼰有限的知识来对某种情况说出 *尽可能* 多的东西。事实上，任何⼀个判断本质上都是⼀种猜测。同样，任何物理理论都是⼀种猜测，其中有成功的，也有失败的。概率论就是为进⾏较好猜测⽽产⽣的⼀种理论体系。应⽤概率的语⾔能使我们定量地谈论某些情况，⽽这些情况的变化可能很⼤，但确有某种⼀贯的平均⾏为。

让我们来研究向上抛掷硬币这件事。如果抛掷——以及硬币本⾝——都是“可靠”的，那么对任何⼀次特定的抛掷，我们⽆法预期能得到什么样的结果。然⽽我们可能会感到，在⼤量的抛掷中应该得到数⽬⼤致相等的正⾯和反⾯。我们说：“每次抛掷以正⾯落地的概率是 $0.5$。”

我们只对将来要做的那些观察谈论概率。*所谓在⼀次观察中将得到⼀个特定结果的 “概率 ”，就是指我们在⼤量重复这个观察时对其中出现该特定结果的最可能分数的估计*。如果我们设想重复作某种观察——⽐如看⼀下刚抛掷的硬币——$N$ 次，并且称 $N_A$ 为我们对这些观察中最可能出现某⼀指定结果 $A$——⽐如出现“正⾯”——的数的 *估计* ，那么所谓观察到 $A$ 的概率 $P(A)$ 就是指
$$\begin{equation}
P(A)=N_A/N.
\end{equation}\tag{6.1}$$
对我们这个定义，需要作⼏点注释。⾸先，只有当所发⽣的事件是某⼀ *可重复* 的观察的可能结果时，我们才能谈到发⽣某件事的概率。像“那所房⼦⾥出现⼀个幽灵的概率是多少？”这类问题有没有任何意义，是不清楚的。

也许你会反对说，没有⼀种情况是 *严格* 重复的。没错。每个不同的观察⾄少要在不同的时间或者地点进⾏的。我们所能说的只是，对于我们想要达到的⽬的来说，凡是重复进⾏的观察应该 *看来似乎都是等价的* 。⾄少我们应当这样假定，每⼀次观察都在同样准备好的情况下进⾏，特别是在观察开始时都要带有同等程度的⽆知（玩纸牌时，如果我们偷看⼀下对⽅的牌，那么我们对⾃⼰获胜的机会的估计就显然与偷看前不同）。

我们应当强调指出，式（6.1）中的 $N$ 和 $N_A$ 并 *不代表* 实际所作观察的次数。$N_A$ 是我们在 $N$ 个 *想象* 的观察中 *可能* 得出结果 $A$ 的观察的最佳 *估计* 。因此，概率有赖于我们的知识以及进⾏估计的能⼒，实际上有赖于我们的常识！幸好许多事物在常识上都有某种程度的⼀致性，所以不同的⼈会作出同样的估计。然⽽，概率不必是⼀些“绝对”的数字。既然它们与我们对事物的⽆知有关，那么如果我们所掌握的知识发⽣变化，它们也会变得不同。

你们也许已经注意到我们的概率定义中另⼀个相当“主观”的⽅⾯。我们把 $N_A$ 说成是对最可能次数的⼀个估计……。可是这并不意味着我们 *不折不扣* 地期望能观察到 $N_A$，⽽是期望能得到⼀个 *靠近*  $N_A$ 的数，⽽且数 $N_A$ ⽐其邻近的任何其他的数 *更为可能* 。⽐如说，我们抛掷⼀个硬币 30 次，那么我们可以预料，得到正⾯的数字不⼤可能正好是 15，⽽很可能是某⼀靠近 15 的数，如 12、13、14、15、16 或 17。然⽽，如果我们 *必须* 对之作出抉择，那么我们就会决定，15 次正⾯要⽐任何其他的数 *更为可取* 。我们将写成：$P(\text{正⾯})=0.5$。

为什么我们选择 15 为⼀个⽐任何其他数更可取的数呢？我们⼀定跟⾃⼰进⾏过如下的争辩：如果在 $N$ 次抛掷中得到正⾯的最可能次数为 $N_H$，那么得到反⾯的最可能次数 $N_T$ 就等于 $N - N_H$ （这⾥我们作了这样的假定，即每次抛掷 *不是* 得到正⾯ *便是* 得到反⾯，不会得到“其他”结果）。但如果硬币是“可靠”的，它就既不偏向正⾯，也不偏向反⾯。除⾮有某些理由可以认为硬币（或者抛掷）是不可靠的，我们就必须认为正⾯与反⾯具有相等的可能性。所以必须使 $N_T = N_H$。这样就得到 $N_T=N_H=N/2$, 或 $P(H)=P(T)=0.5$.

我们可以把这⼀论证推⼴到 *任何* ⼀种情况，在这种情况下，可以观察到 $m$ 个不同但又“相等”（即机会均等）的可能的结果。如果通过观察能得出 $m$ 个不同结果，⽽且又有理由相信，其中任何⼀个结果与别的任何结果同样可能，那么得到某⼀个 *特定* 结果 $A$ 的概率就等于 $P(A)=1/m$。

如果在⼀个不透明的箱⼦⾥有 7 个不同颜⾊的⼩球，我们“随便”（即不朝它看时）取出⼀个，那么得到某⼀种颜⾊的⼩球的概率是 $\tfrac{1}{7}$。从已洗过的 52 张牌中“任意”抽出⼀张红桃 10 的概率是 $\tfrac{1}{52}$。掷骰⼦⽽得到两个⼀点的概率是 $\tfrac{1}{36}$。

在第 5 章中，我们⽤原⼦核的表观⾯积，或者称为“截⾯”来描写它的⼤⼩。这样做时，实际上我们就是在谈概率。当我们向⼀块薄的材料发射⼀个⾼能粒⼦时，它有⼀定机会直接穿过去，也有⼀定机会碰撞在⼀个原⼦核上（既然原⼦核如此之⼩，以致我们⽆法看到，我们就不可能直接瞄准，⽽必须“盲⽬射击”）。设在这块薄板中有 $n$ 个原⼦，⽽每个原⼦的核具有截⾯积 $\sigma$，那么被所有这些核所“遮盖”的总⾯积为 $n\sigma$。在随机发射的很⼤数⽬ $N$ 中，我们预期能击中某些核的数⽬ $N_C$ 与 $N$ 之⽐，犹如被遮盖的⾯积与薄板的总⾯积之⽐
$$\begin{equation}
N_C/N=n\sigma/A.
\end{equation} \tag{6.2}$$
因此我们可以说，任何⼀个⼊射粒⼦在穿过薄板时将经受⼀次撞击的 *概率* 为
$$\begin{equation}
P_C=\frac{n}{A}\,\sigma,
\end{equation}\tag{6.3}$$
其中 $n/A$ 是我们这块薄板中单位⾯积内的原⼦数。


## 6–2、涨落

![[f06-01_tc_big.png|300]]

**图 6–1：** 在每轮为 30 次抛掷的三轮游戏中所观察到的正面和反面的前后次序


我们现在想利⽤有关概率的概念来⽐较详细地考虑⼀下这样的⼀个问题：“如果我把⼀个硬币抛掷 $N$ 次，那么 *预期* 会得到多少次真正的正⾯？”然⽽在回答这个问题之前，让我们先来看⼀下在这样⼀个“实验”中确实会发⽣什么情况。图 6-1 表示 $N=30$ 的这样⼀个实验在前三“轮”中所得的结果。“正⾯”和“反⾯”的前后次序完全是按照它们得到时的次序排列的。第⼀轮得到 11 次正⾯；第⼆轮也是 11 次；第三轮 16 次。在这三轮试验中，我们没有⼀回得到 15 次正⾯。是不是要对硬币开始发⽣怀疑呢？或者在这样⼀种游戏中，我们设想得到正⾯的最可能次数是 15 这⼀点错了呢？再做 97 轮实验，以便⼀共得到 $100$ 轮每回抛掷 30 次的实验。实验的结果列在表 6-1 中[^1]。

**表 6–1：** 在抛掷一个硬币 $30$ 次的逐轮试验中每轮所得正面的数目

$$\left. \begin{array}{cccccccccc} 11 & 16 & 17 & 15 & 17 & 16 & 19 & 18 & 15 & 13 \\ 11 & 17 & 17 & 12 & 20 & 23 & 11 & 16 & 17 & 14 \\ 16 & 12 & 15 & 10 & 18 & 17 & 13 & 15 & 14 & 15 \\ 16 & 12 & 11 & 22 & 12 & 20 & 12 & 15 & 16 & 12 \\ 16 & 10 & 15 & 13 & 14 & 16 & 15 & 16 & 13 & 18 \\ 14 & 14 & 13 & 16 & 15 & 19 & 21 & 14 & 12 & 15 \\ 16 & 11 & 16 & 14 & 17 & 14 & 11 & 16 & 17 & 16 \\ 19 & 15 & 14 & 12 & 18 & 15 & 14 & 21 & 11 & 16 \\ 17 & 17 & 12 & 13 & 14 & 17 & 9 & 13 & 19 & 13 \\ 14 & 12 & 15 & 17 & 14 & 10 & 17 & 17 & 12 & 11 \\ \end{array} \right\} \text{100 trials}$$

如果观察⼀下表 6-1 中所列的各数，那么我们看到，⼤多数结果“靠近” 15，⽽且位于 12 与 18 之间。如果我们为这些结果画⼀张 *分布* 图，那么就会对这些结果的细节有⼀个更好的理解。我们计算⼀下得到某⼀记录 $k$ 的实验次数，并把这个数对每⼀个 $k$ 作图，如图 6-2 所示。记录到 15 次正⾯的共有 13 轮游戏。记录到 14 次正⾯的也是 13 轮。得到 16 和 17 次的，每⼀个都 *⼤于* 13 轮。我们是否断定这⾥对正⾯有所偏袒？我们的“最佳估计”是否不够好？是不是我们现在应该作出这个结论，即每轮 30 次抛掷的“最可能”记录实际上是 16 次正⾯？但是且慢！把所有各轮游戏加到⼀起，就总共抛掷了 $3000$ 次。⽽获得正⾯的总数是 $1493$。可见出现正⾯的抛掷其⽐数是 $0.498$，很接近⽽ *稍⼩于* $0.5$。当然我们 *不应* 假定抛掷后得到正⾯的概率⼤于 $0.5$！⾄于某 *特定* 的⼀组观察经常得到 16 次正⾯这个事实，是⼀种 *涨落* 现象。然⽽我们仍然预期 *最可能* 的正⾯数是 15。

![[f06-02_tc_big.png|500]]

**图 6–2：** 每轮 $30$ 次抛掷的 $100$ 轮游戏所得结果的概况。垂直线表示记录到 $k$ 次正面的各轮游戏的数目。虚线表示从概率计算求得的所期望记录到 $k$ 次的游戏轮数

我们可以提出这样的⼀个问题：“在 30 次抛掷的游戏中将获得 15、16 或任何其他次数正⾯的概率 *是* 多少？”我们已经说过，在抛掷⼀次的游戏中，得到 *⼀次* 正⾯的概率是 $0.5$，得不到正⾯的概率也是 $0.5$。在抛掷两次的游戏中，有 *四种* 可能的结果：即 $HH$，$HT$，$TH$，$TT$。既然这些结果中的每⼀个都是同样可能的，我们就推断出：（a）记录到两次正⾯的概率是 $\tfrac{1}{4}$；（b）记录到⼀次正⾯的概率是 $\tfrac{2}{4}$；（c）记录到零次正⾯的概率是 $\tfrac{1}{4}$。这⾥有 *两种* ⽅式可以得到⼀次正⾯。但是得到两次或零次正⾯的⽅式各只有⼀种。

 现在我们来研究抛掷三次的游戏。第三次抛掷同样可能得到⼀个正⾯或者⼀个反⾯。这⾥得到三次正⾯的⽅式只有⼀种：我们 *必须* 在前两次抛掷中得到两次正⾯，⽽后在最后⼀次中也得到正⾯。可是这⾥有 *三种* ⽅式可以得到两次正⾯。在掷得两次正⾯（⼀种⽅式）后，我们可以掷出反⾯，或者在前两次抛掷中只掷出⼀次正⾯（两种⽅式）后，我们可以掷出⼀个正⾯。因此对于 $3—H$，$2—H$，$1—H$，$0—H$ 等记录，其同样可能的⽅式的数⽬分别为 1，3，3，1。共有⼋种不同的可能结果。于是其概率分别为 $\tfrac{1}{8}$, $\tfrac{3}{8}$, $\tfrac{3}{8}$, $\tfrac{1}{8}$。

![[f06-03_tc_big.png|400]]
  
**图 6–3：** 在抛掷三次的游戏中，能得到 0，1，2，3 次正面的方式数目的图解表示

![[f06-04_tc_big.png|300]]

**图 6–4：** 类似于图 6-3 的抛掷 6 次的游戏的图解表示


刚才的讨论可以⽤图 6-3 所示的图解表示来概括。可以清楚看出，对于更⼤数⽬的抛掷，应如何来把这个图解表示继续下去。图 6-4 表示抛掷 6 次的这样⼀个图解表示。达到图中任何⼀点的所有“⽅式”的数⽬就是从起点开始到该点可以取的各种不同“途径”（即正⾯和反⾯相连的各种次序）的数⽬。最后⼀栏告诉我们掷得正⾯的总数。这样⼀种图表中出现的⼀组数称为 *帕斯卡三角形*。这些数也称为 *⼆项式系数* ，因为它们也出现在 $(a+b)^n$ 的展开式中。如果我们称 $n$ 为抛掷的次数，$k$ 为掷得正⾯的次数，那么图表中的数字通常⽤符号 $\tbinom{n}{k}$ 来表示。顺便提⼀下，⼆项式系数也可以从下式
$$\begin{equation}
\binom{n}{k}=\frac{n!}{k!(n-k)!},
\end{equation}$$
算出，其中 $n!$ 称为“$n$ 阶乘”，表示连乘积 $(n)(n-1)(n-2)\dotsm(3)(2)(1)$ 的意思。

我们现在打算根据式（6.1）来计算在 $n$ 次抛掷中得到 $k$ 次正⾯的概率 $P(k,n)$。所有可能结果的总数是 $2^n$（因为对每⼀抛掷有两个结果），得到 $k$ 次正⾯的总共有 $\tbinom{n}{k}$ 种，⽽每⼀种都是同样可能的，所以我们有
$$\begin{equation}
P(k,n)=\frac{\tbinom{n}{k}}{2^n}.
\end{equation}\tag{6.5}$$
既然 $P(k,n)$ 是我们期望会得到 $k$ 次正⾯的⽐数，那么在 $100$ 轮游戏中，我们应预期共有 $100\cdot P(k,n)$ 轮会出现 $k$ 次正⾯。图 6-2 中虚线所经过的各点就是从 $100\cdot P(k,30)$ 计算出来的那些点⼦。我们可以看到，我们 *预期* 有 14 或 15 轮游戏会记录到 15 次正⾯，然⽽只有 13 轮游戏观察到这个记录，我们 *预期* 有 13 或 14 轮游戏会记录到 16 次正⾯，但是却有 16 轮游戏观察到这个记录。这种涨落情况是“游戏的组成部分”。

> [!tip]
> $P(15,30)$ 的计算值如下：$$P(15,30) = \frac{\frac{30!}{15!(30-15)!}}{2^{30}} =0.14446$$
> $P(16,30)$ 的计算值如下：$$P(16,30) = \frac{\frac{30!}{16!(30-16)!}}{2^{30}} =0.13544$$
> 

我们刚才⽤过的⽅法，可以应⽤于最⼀般的情况，也就是在单独⼀次观察中只能得出两种可能结果的情况。我们⽤ $W$［表示“win”（赢）］和 $L$［表示“lose”（输）］来表示这两种结果。在⼀般情况下，单独⼀个事件会得 $W$ 或 $L$ 的概率是⽆需相等的。设 $p$ 为得到结果 $W$ 的概率。于是 $q$ ——这个得到结果 $L$ 的概率必然等于 $(1-p)$。在⼀组 $n$ 轮的试验中，得到 $k$ 次结果为 $W$ 的概率 $P(k,n)$ 就等于
$$\begin{equation}
P(k,n)=\tbinom{n}{k}p^kq^{n-k}.
\end{equation} \tag{6.6}$$
这个概率函数称为 *伯努利* 或 *⼆项式概率* 。

## 6–3、⽆规⾏⾛

另⼀个有趣的问题也需要⽤到概率概念。这就是“⽆规⾏⾛”的问题。在最简单的形式下，我们可以想象这样⼀个“游戏”，其中“游戏者”从 $x =0$ 的⼀点出发，要求他每“移动”⼀次 *要么* 朝前（向 $+x$ ⽅向）⾛⼀步，*要么* 朝后（向 $-x$ ⽅向）⾛⼀步。⽽朝前朝后必须 *随机* 决定，例如⽤抛掷硬币的⽅法。我们将怎样来描写这种运动的结果呢？在⼀般形式下，这个问题与⽓体中原⼦（或其他粒⼦）的运动，即布朗运动有关，也与测量中误差的组合有关。你们将会看到，⽆规⾏⾛问题与我们已讨论过的抛掷硬币问题密切有关。

⾸先，让我们看⼏个⽆规⾏⾛的例⼦。我们可以⽤⾏⾛者在 $N$ 步中所经过的净距离 $D_N$ 来表示他的进度。图 6-5 为⽆规⾏⾛者所⾛路径的三个例⼦（这⾥我们⽤图 6-1 所示抛掷硬币所得的结果作为随机选择的移动取向）。

![[Pasted image 20241112153936.png|400]]

**图 6–5：** 无规行走取得的进度。横坐标 $N$ 表示所走的步子总数；纵坐标 $D_N$ 表示离开起点的净距离


对于这样⼀种移动我们可以说些什么呢？⾸先我们也许会问：“平均⽽⾔他⾛了多远？”我们必定 *预期* 他的平均进度将为零，因为他向前或向后⾛的可能性是均等的。然⽽我们有这样的感觉，随着 $N$ 的增加，他更可能偏离起点越来越远。因此我们也许要问，⾛过的⽤ *绝对值* 表示的平均距离是多少，也就是说 $\lvert{D}\rvert$ 的平均值是多少。可是在这⾥⽤另⼀种量度“进度”的⽅法更为⽅便。这就是⽤距离的平⽅ $D^2$ 来表示，它⽆论对正的还是负的移动都为正，所以它是这种随机漫步的⼀个合理 *量度* 。

我们可以证明，$D_N^2$ 的预期值恰好是所⾛步⼦的数⽬ $N$。所谓“预期值”，指的是可⼏值（也就是我们的最佳猜测），我们可以把它看作是对 *重复多次* 的⼀系列⾏⾛所 *预期* 的平均⾏为。我们⽤ $\langle{D_N^2}\rangle$ 来表示这样⼀个预期值，并且也可以称它为“⽅均距离”。⾛⼀步后的 $D^2$ 总是 $+1$，所以当然（所有的距离都将以⼀步为单位来量度。以后我们将不再写出距离的单位）。

当 $N>1$ 时，预期值 $D_N^2$ 可以从 $D_{N-1}$ 求得。如果⾛了 $(N-1)$ 步后，我们得到 $D_{N-1}$，那么经过 $N$ 步后，就有 $D_N=D_{N-1}+1$ 或 $D_N=D_{N-1}-1$。其平⽅为
$$\begin{equation}
D_N^2=
\begin{cases}
D_{N-1}^2+2D_{N-1}+1,\\[2ex]
\kern{3.7em}\textit{or}\\[2ex]
D_{N-1}^2-2D_{N-1}+1.
\end{cases}
\end{equation}\tag{6.7}$$
对于⼤量独⽴的⽆规⾏⾛，我们所能预期得到的，每次只有每⼀个数值的⼀半，因此我们的平均预期值恰好是这两个可能值的平均值。于是 $D_N^2$ 的预期值就是 $D_{N-1}^2+1$。*⼀般⽽⾔*，我们对 $D_{N-1}^2$ 所应 *期望* 的“预期值”就是 $\langle{D_{N-1}^2}\rangle$（根据定义！）。所以$$\begin{equation}
\langle{D_N^2}\rangle=\langle{D_{N-1}^2}\rangle+1.
\end{equation} \tag{6.8}$$我们已经说明 ；因⽽得到
$$\begin{equation}
\langle{D_N^2}\rangle=N,
\end{equation}\tag{6.9}$$
这是⼀个多么简单的结果！

如果我们希望得到的不是距离的平⽅，⽽是像距离那样的⼀个数，以表示⽆规⾏⾛中“所作的从原点算起的进展”，那么我们可以⽤“⽅均根距离” $D_{\text{rms}}$ 来表示：
$$\begin{equation}
D_{\text{rms}}=\sqrt{\langle{D^2}\rangle}=\sqrt{N}.
\end{equation}\tag{6.10}$$
> [!tip]
> 在无规行走中，$D$ 指的是最终偏离原点的净距离，也就是比如往右走的距离-往左走的距离；这就像抛银币，正面-反面的总距离

我们已经指出，⽆规⾏⾛问题在数学形式上与本章开始时讨论过的那种抛掷硬币的游戏⼗分相似。如果我们设想每⼀步的取向对应于抛掷硬币中出现的正⾯或反⾯，那么 $D$ 正好是获得正⾯的次数与获得反⾯的次数的差值 $N_H-N_T$。由于 $N_H+N_T=N$ 是总的所⾛步数（或总的所抛掷次数），我们就有 $D=2N_H-N$。以前我们曾为预期的分布 $N_H$ （也称为 $k$）导出⼀个表达式，⽽且得到了如式（6.5）所示的结果。由于 $N$ 正好是⼀个常数，所以我们就为 $D$ 得到⼀个相应的分布（由于超过 $N/2$ 后出现的每次正⾯都会使反⾯受到“损失”，所以在 $N_H$ 与 $D$ 之间相差⼀个因⼦ 2）。图 6-2 表示在⽆规⾏⾛ 30 步的例⼦中可能得到的距离分布情况（其中 $k=15$ 应读作 $D=0$，$k=16$ 应读作 $D=2$，等等）。

$N_H$ 和它的预期值 $N/2$ 的偏差为$$\begin{equation}
N_H-\frac{N}{2}=\frac{D}{2}.
\end{equation}\tag{6.11}$$⽅均根（rms）偏差为$$\begin{equation}
\biggl(N_H-\frac{N}{2}\biggr)_{\text{rms}}=\tfrac{1}{2}\sqrt{N}.
\end{equation}\tag{6.12}$$根据我们对 $D_{\text{rms}}$ 求得的结果，在⾛ 30 步所预期的“典型”距离应是 $\sqrt{30} \approx 5.5$，或者典型的 $k$ 应与 15 相差⼤约 $5.5/2 = 2.75$ 个单位。在图 6-2 中，我们可以看到，从中⼼量起的曲线“宽度”正好⼤约等于 $3$ 个单位，和上述结果相⼀致。

现在我们已有条件来考虑⼀直到⽬前为⽌被我们所回避的⼀个问题。我们怎样知道⼀块硬币是“可靠的”或是“灌过铅的”？现在我们⾄少能够为之提供⼀部分答案。对于⼀块可靠的硬币，我们预期其能出现正⾯的次数的⽐值是 $0.5$，亦即$$\begin{equation}
\frac{\langle{N_H}\rangle}{N}=0.5.
\end{equation}\tag{6.13}$$我们也预期实际的 $N_H$ 将偏离 $N/2$ ⼤约有 $\sqrt{N}/2$，或者说，它的⽐值与 $1/2$ 的偏差为
$$\begin{equation*}
\frac{1}{N}\,\frac{\sqrt{N}}{2}=\frac{1}{2\sqrt{N}}.
\end{equation*}$$
> [!tip]
> 真实的 $N_H = \frac{N}{2} \pm \frac{\sqrt{N}}{2}$ 次，其概率为 $\frac{N_H}{N}$，与 $\frac{1}{2}$ 的偏差为 $\frac{N \pm \sqrt{N}}{2N}-\frac{1}{2}=\pm\frac{1}{2\sqrt{N}}$

$N$ 越⼤，所预期的⽐值 $N_H/N$ 就越接近于⼆分之⼀。

> [!tip]
> 显而易见，$N$ 越大，分母接近于 0，即偏置为 0

![[Pasted image 20241112160139.png|400]]

**图 6–6：** 在一连串 $N$ 次抛掷中获得正面次数的比例


在图 6-6 中，我们根据本章前⾯提到的掷币记录画了⼀条表示⽐值 $N_H/N$ 的曲线。从图中可以看出，对于⼤的 $N$ ，得正⾯的⽐值趋向于接近 $0.5$。遗憾的是，对任何给定的⼀轮或⼏轮，连观察到的偏差都 *保证* 不了 *接近* 于 *预期* 的偏差。总是有⼀定的机会出现⼤的涨落——⼀长串的正⾯或者⼀长串的反⾯，造成⼀个任意⼤的偏差。我们⼀切所能说的，只是如果偏差接近于预期的 $1/2\sqrt{N}$（⽐如说在 $2$ 或 $3$ 倍之内），那么就没有理由去怀疑硬币的可靠性。如果偏差⼤得多，那么我们可以对硬币发⽣怀疑，但⽆法证明它是灌过铅的（或者抛掷者是⾮常机灵的）。

我们也没有考虑过应该如何来处理这样⼀块“硬币”或某⼀与之相似的“不确定的”物体（⽐如⼀块始终以两种⽅位中⽆论哪⼀种着地的⽯块），对于它们来说，我们很有理由认为出现正⾯和反⾯的概率 *应该* 是不同的。我们已经定义了 $P(H)=\langle{N_H}\rangle/N$。那么怎样知道 $N_H$ 的 *预期* 值是多少呢？在某些情况下，我们所能做得最好的，就是去观察在⼤量抛掷中所得正⾯的数⽬。由于缺少任何更好的数据，我们不得不令 $\langle{N_H}\rangle=N_H(\text{观察值})$（除此之外，还能期望做什么呢）。然⽽必须理解到，在这样⼀种情况下，不同的实验或不同的观察者可能会推论出不同的概率 $P(H)$。但是我们可以 *预料* ，这些不同的答案应该在偏差 $1/2\sqrt{N}$ 的范围内相互⼀致［假如 $P(H)$ 接近于 $1/2$ 的话］。实验物理学家常常这样说：“实验确定的”概率是有“误差”的，并且把它写成$$\begin{equation}
P(H)=\frac{N_H}{N}\pm\frac{1}{2\sqrt{N}}.
\end{equation} \tag{6.14}$$在这样⼀个表示式中含有下列意义：存在着⼀个“真正的”或“正确的”概率，只要我们知道的东西⾜够多，就 *能* 把它计算出来，其次是由于有涨落，观察会发⽣“误差”。然⽽没有办法能使这种想法做到逻辑上始终如⼀。如果能领悟到下列⼏点或许要⽐较好⼀些，即概率概念在某种意义上是主观的，它总是建⽴在不肯定的知识上的，⽽且它的定量值是随着我们得到的信息越多⽽改变着。

## 6–4、概率分布

我们现在回到⽆规⾏⾛的问题上来，并且考虑它的⼀种修正。我们设想除了每⼀步的 *⽅向*（$+$ 或 $-$）可以随机选择外，每⼀步的 *长度* 也能以某种⽆法预定的⽅式变化着，唯⼀的条件就是 *平均* ⽽⾔步⼦的长度是⼀个单位。这种情况更能代表象⽓体中⼀个分⼦的热运动那样的状况。如果我们称⼀步的长度为 $S$，那么 $S$ 完全可以取任何⼀个值，但最通常的是“接近于” 1。为明确起见，我们令 $\langle{S^2}\rangle=1$，或者与之同等，$S_{\text{rms}}=1$。$\langle{D^2}\rangle$ 的推导将仿照以前⼀样，只是式（6.8）现在要加以改变⽽写成
$$\begin{equation}
\langle{D_N^2}\rangle=\langle{D_{N-1}^2}\rangle+\langle{S^2}\rangle=\langle{D_{N-1}^2}\rangle+1.
\end{equation}\tag{6.15}$$
同以前⼀样，我们得到$$\begin{equation}
\langle{D_N^2}\rangle=N.
\end{equation}\tag{6.16}$$现在对于距离 $D$，我们会预期得到什么样的⼀种分布呢？⽐如在⾛了 30 步后，$D=0$ 的概率是多少？回答是 $0$！$D$ 取 *任⼀特定* 值的概率是 $0$，因为根本没有⼀种机会能使后退的（长度是变化的）步⼦的总和与朝前的步⼦的总和正好相等。我们⽆法画出⼀张像图 6-2 那样的图。

然⽽如果我们不是去问获得其值正好等于 $0$，$1$，或 $2$ 的那些 $D$ 的概率是多少，⽽代之以去问获得其值靠近 $0$，$1$，或 $2$ 的那些 $D$ 的概率有多⼤，那么我们就能得到与图 6-2 相似的曲线。我们定义 $P(x,\Delta x)$ 为 $D$ 位于 $x$ 处⼀个间隔 $\Delta x$（⽐如从 $x$ 到 $x+\Delta x$）内的概率。对于⼩的 $\Delta x$，我们可以预期 $D$ 位于这个间隔内的概率，与间隔的宽度 $\Delta x$ 成正⽐。因此我们可以写成
$$\begin{equation}
P(x,\Delta x)=p(x)\,\Delta x.
\end{equation}\tag{6.17}$$
函数 $p(x)$ 称为 *概率密度* 。

$p(x)$ 的形式与所⾛步⼦的数⽬ $N$ 有关，也与个别步⼦的长度分布有关。我们不能在这⾥给出有关的论证，但当 $N$ 很⼤时，对于所有合理的个别步⼦的长度分布，$p(x)$ 都是相同的，因⽽只取决于 $N$。在图 6-7 中，我们对三个 $N$ 值各作⼀条曲线。你们会注意到，这些曲线的“半宽度”（离 $x=0$ 的典型散布范围）是 $\sqrt{N}$，正如我们已证明过它理应如此。

![[Pasted image 20241112163656.png|400]]
  
**图 6–7：** 在步数为 $N$ 的无规行走中终止在从起点算起的距离 $D$ 处的概率密度（$D$ 是用均方根步长为单位来量度的）

> [!tip]
> 在高斯分布曲线中，概率密度函数（PDF）可以表示为：$$f(x|\mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$其中，$\mu$ 是均值，$\sigma$ 是标准差，$\sigma^2$ 是方差。
> 
> 最高点处为，当 $x=\mu$ 时的概率密度函数的值，此时公式 $e$ 的指数项为 0，即概率值为：$$f(\mu|\mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}$$显然这个值与标准差 $\sigma$ 成反比。随着标准差的增大，曲线变得更宽且不那么高；随着标准差的减小，曲线变得更瘦且更高。换句话说，标准差越大，分布越分散；标准差越小，分布越集中。

你们可能也已注意到，靠近零处的 $p(x)$ 值反⽐于 $\sqrt{N}$。这是由于曲线都有相似的形状以及曲线下⾯的⾯积都应相等⽽来的。既然 $p(x)\,\Delta x$ 是当 $\Delta x$ 很⼩时在 $\Delta x$ 中找到 $D$ 的概率，那么我们可以这样来确定在任意⼀个从 $x_1$ 到 $x_2$ 的间隔内不论何处找到 $D$ 的概率，只要把间隔分割成许多微⼩增量 $\Delta x$，然后对每个增量的有关概率 $p(x)\,\Delta x$ 相加⽽求其总和。$D$ 落在 $x_1$ 与 $x_2$ 之间某处的概率，我们可以写作 $P(x_1 < D < x_2)$，它等于图 6-8 中所示阴影的⾯积。增量 $\Delta x$ 取得越⼩，结果就越正确。因此我们可以写成
$$\begin{equation}
\begin{gathered}
P(x_1 < D < x_2)=\sum p(x)\Delta x\\[1ex]
=\int_{x_1}^{x_2}p(x)\,dx.
\end{gathered}
\end{equation} \tag{6.18}$$
![[Pasted image 20241112164123.png|300]]
**图 6–8：** 无规行走中所通过的距离 $D$，它位于 $x_1$ 与 $x_2$ 之间的概率就是曲线 $p(x)$ 下面从 $x_1$ 到 $x_2$ 的面积


整个曲线下⾯的⾯积是 $D$ 落在不论何处（也就是它具有在 $x=-\infty$ 到 $x=+\infty$ 之间的某⼀值）的概率。这个概率当然是 1。因⽽必须有
$$\begin{equation}
\int_{-\infty}^{+\infty}p(x)\,dx=1.
\end{equation} \tag{6.19}$$
由于图 6-7 中的曲线与 $\sqrt{N}$ 成⽐例地变宽，所以为了保持总⾯积等于 $1$，它们的⾼度必须正⽐于 $1/\sqrt{N}$。

我们这⾥所描述的概率密度函数是最经常遇到的⼀种。通常把它称为 *正态* 或 *⾼斯概率密度*。它的数学形式是$$\begin{equation}
p(x)=\frac{1}{\sigma\sqrt{2\pi}}\,e^{-x^2/2\sigma^2},
\end{equation} \tag{6.20}$$其中 $\sigma$ 称为 *标准偏差* ，在我们的情况中 $\sigma=\sqrt{N}$，或者当⽅均根步长不为 $1$ 时 $\sigma=\sqrt{N}S_{\text{rms}}$

前⾯我们已提到，⽓体中⼀个分⼦或任何⼀个粒⼦的运动犹如⼀种⽆规⾏⾛。假定我们打开⼀个装着有机化合物的瓶⼦，让它的⼀部分蒸⽓跑到空⽓中去。如果外⾯有⽓流，以致空⽓在作循环运动，那么⽓流也将带着蒸⽓⼀起运动。然⽽即使在 *完全静⽌的空⽓* 中，蒸⽓也会渐渐散布开去，进⾏扩散，直到布满整个房间。我们可以从它的颜⾊或⽓味加以鉴别。有机化合物蒸⽓的个别分⼦之所以能在静⽌空⽓中散布开去，是由于这些分⼦与其他分⼦碰撞⽽造成的分⼦运动所致。如果我们知道其“步⼦”的平均⼤⼩，以及每秒所⾛的步数，那么就能求出⼀个或 $n$ 个分⼦在经过任何⼀段特定时间后在从其起点算起的某⼀距离被找到的概率。随着时间的消逝，步⼦越⾛越多，⽓体就会像图 6-7 中相继的⼏条曲线那样逐渐散开。在以后要讲的⼀章中，我们将求出步⼦的⼤⼩和步⼦的频率如何与⽓体的温度和压强有关。

我们以前说过，⽓体的压强是由于分⼦撞击容器壁⽽形成的。以后如果要作较定量的描写时，我们就需要知道分⼦在弹跳时跑得有多快，因为它们所作的碰撞与这个速率有关。然⽽我们不能说这些分⼦具有如何确定的速率。这⾥必须⽤概率来描写。⼀个分⼦可以具有任何⼀个速率，但有些速率出现的可能性⽐另⼀些要⼤。我们可以这样来描写⽓体内正在发⽣什么，这就是说出任何⼀个特定分⼦具有速率在 $v$ 与 $v+\Delta v$ 之间的概率 $p(v)\,\Delta v$，⽽ $p(v)$ 这个概率密度是速率 $v$ 的⼀个确定函数。往后我们会看到，麦克斯韦如何运⽤常识和概率观念为 $p(v)$ 找到⼀个数学表示式。函数 $p(v)$ 的形状如图 6-9 所示。速度可以取任何⼀个值，但是最可能取的是靠近最可⼏值或 $v_p$ 的那⼀些。

![[Pasted image 20241112165014.png|300]]
**图 6–9：** 气体中分子的速度分布


我们常常以稍微不同的⽅式去看待图 6-9 中的曲线。如果我们考虑⼀个典型容器（⽐如，其体积为 $1 \text{ l}$）中的分⼦，那么容器中存在着极⼤数量的分⼦（$N\approx10^{22}$）。由于 $p(v)\,\Delta v$ 是⼀个分⼦具有在 $\Delta v$ 间隔内的速度的概率，所以根据我们对概率的定义，我们说，找到速度处在间隔 $\Delta v$ 内的分⼦数的预期值 $\langle{\Delta N}\rangle$ 应是
$$\begin{equation}
\langle{\Delta N}\rangle=N\,p(v)\,\Delta v.
\end{equation} \tag{6.21}$$
我们称 $N\,p(v)$ 为“速度分布”。曲线下⾯两个速度 $v_1$ 与 $v_2$ 之间的⾯积，例如图 6-9 中所示阴影的⾯积代表了［对曲线 $N\,p(v)$ 来说］速度在 $v_1$ 和 $v_2$ 之间的分⼦的预期数。由于在⽓体的情况中，我们通常与⼤量的分⼦打交道，所以可以期望这⼀⾯积与预期数的偏差是⼩的（犹如 $1/\sqrt{N}$），因此我们常常不说“预期”数，⽽代之以说：“具有速度在 $v_1$ 和 $v_2$ 之间的分⼦数是曲线下⾯的⾯积。”但是我们应当记住，这种陈述所谈到的总是可⼏数。

## 6–5、不确定性原理

<div id="Ch6-S5-p1" class="para">
<p class="p">The ideas of probability are certainly useful in describing the behavior
of the&nbsp;$10^{22}$ or so molecules in a sample of a gas, for it is clearly
impractical even to attempt to write down the position or velocity of
each molecule. When probability was first applied to such problems, it
was considered to be a <em class="emph">convenience</em>—a way of dealing with very
complex situations. We now believe that the ideas of probability are
<em class="emph">essential</em> to a description of atomic happenings. According to
quantum mechanics, the mathematical theory of
particles, there is always some uncertainty in the <em class="emph">specification</em>
of positions and velocities. We can, at best, say that there is a
certain probability that any particle will have a position near some
coordinate&nbsp;$x$.</p>
<p class="p blue-text">
在描写⽓体样品中 $10^{22}$ 个或类似这样多个分⼦的⾏为时，概率的概念肯定是有⽤的，因为很清楚，即使要写下每个分⼦的位置或速度这种试图，也是不实际的。当概率最初运⽤于这类问题时，⼤家曾认为这是⼀种⽅便——⼀种处理⾮常复杂的情况的⽅法。现在我们认为，概率的概念是描写原⼦事件所必不可少的。按照量⼦⼒学这个有关粒⼦的数学理论，在说明位置和速度⽅⾯总是存在着某种不确定性。充其量我们可以说，任何粒⼦只有⼀定的概率可以使它的位置接近某⼀坐标 $x$。
</p>
</div>
<div id="Ch6-S5-p2" class="para">
<p class="p">We can give a probability density&nbsp;$p_1(x)$, such that&nbsp;$p_1(x)\,\Delta x$
is the probability that the particle will be found between $x$
and&nbsp;$x+\Delta x$. If the particle is reasonably well localized, say
near&nbsp;$x_0$, the function&nbsp;$p_1(x)$ might be given by the graph of
Fig.&nbsp;<a href="#Ch6-F10" class="">6–10</a>(a). Similarly, we must specify the velocity of the
particle by means of a probability density&nbsp;$p_2(v)$,
with&nbsp;$p_2(v)\,\Delta v$ the probability that the velocity will be found
between $v$ and&nbsp;$v+\Delta v$.</p>
<p class="p blue-text">
我们可以这样来引进⼀个概率密度函数 $p_1(x)$，使 $p_1(x)\,\Delta x$ 为在 $x$ 与 $x+\Delta x$ 之间找到这个粒⼦的概率。如果这个粒⼦的位置被很好地限制在某个地⽅，⽐如说靠近 $x_0$，那么函数 $p_1(x)$ 就可能如图 6-10（a）所示的曲线给出的那样。与之相似，我们必须⽤概率密度 $p_2(v)$ 来限定粒⼦的速度，⽽ $p_2(v)\,\Delta v$ 则表示能找到⼀个处于 $v$ 与 $v+\Delta v$ 之间的速度的概率，如图 6-10（b）所示。
</p>
</div>
<div id="Ch6-F10" class="figure multi">
<img class="first" src="./img/I_06/f06-10_tc_big_a.svgz"><img class="last" src="./img/I_06/f06-10_tc_big_b.svgz"><div class="caption">
<span class="tag">图 6–10.</span>观察一个粒子的位置与速度时的概率密度
</div>
</div>
<div id="Ch6-S5-p3" class="para">
<p class="p">It is one of the fundamental results of quantum mechanics that the two functions $p_1(x)$ and&nbsp;$p_2(v)$ cannot be chosen
independently and, in particular, cannot both be made arbitrarily
narrow. If we call the typical “width” of the $p_1(x)$&nbsp;curve $[\Delta
x]$, and that of the $p_2(v)$&nbsp;curve $[\Delta v]$ (as shown in the
figure), nature demands that the <em class="emph">product</em> of the two widths be at
least as big as the number&nbsp;$\hbar/2m$, where $m$ is the mass of the
particle. We may write this basic relationship as</p>
<p class="p blue-text">
量⼦⼒学的基本结果之⼀是：两个函数 $p_1(x)$ 与 $p_2(v)$ 不能予以独⽴选定，特别是不能把它们都取得任意的窄。如果我们称 $p_1(x)$ 曲线的典型“宽度”为 $[\Delta
x]$，$p_2(v)$ 曲线的典型宽度为 $[\Delta v]$（各如图所示），那么⾃然界就要求这两个宽度的乘积⾄少要与数 $\hbar/2m$ ⼀样⼤，这⾥ $m$ 是粒⼦的质量。我们可以把这个基本关系写成
</p>

$$\begin{equation}
\label{Eq:I:6:22}
[\Delta x]\cdot[\Delta v]\geq\hbar/2m.
\end{equation}$$

<p>This equation is a statement of the <em class="emph">Heisenberg uncertainty
principle</em> that we mentioned earlier.</p>
<p class="blue-text">这个式⼦就是我们前⾯提到过的海森伯不确定性原理的⼀种表述。</p>
</div>
<div id="Ch6-S5-p4" class="para">
<p class="p">Since the right-hand side of Eq.&nbsp;(<a href="#mjx-eqn-EqI622" class="">6.22</a>) is a constant, this
equation says that if we try to “pin down” a particle by forcing it to
be at a particular place, it ends up by having a high speed. Or if we
try to force it to go very slowly, or at a precise velocity, it
“spreads out” so that we do not know very well just where it is.
Particles behave in a funny way!
</p>
<p class="p blue-text">
由于式（6.22）的右⾯是⼀个常数，这就表明，如果我们迫使⼀个粒⼦处于某⼀特定位置⽽试图把它“钉住”，结果它就获得⼀个很⼤的速度。或者是：如果我们迫使它跑得很慢，或以精确的速度运动，那么它就要“散开”，以致我们不能很好地知道它究竟在哪⾥。粒⼦的举⽌真是太奇妙了！
</p>
</div>
<div id="Ch6-S5-p5" class="para">
<p class="p">The uncertainty principle describes an inherent fuzziness that must
exist in any attempt to describe nature. Our most precise description of
nature <em class="emph">must</em> be in terms of <em class="emph">probabilities</em>. There are some
people who do not like this way of describing nature. They feel somehow
that if they could only tell what is <em class="emph">really</em> going on with a
particle, they could know its speed and position simultaneously. In the
early days of the development of quantum mechanics, Einstein was quite
worried about this problem. He used to shake his head and say, “But,
surely God does not throw dice in determining how electrons should go!”
He worried about that problem for a long time and he probably never
really reconciled himself to the fact that this is the best description
of nature that one can give. There are still one or two physicists who
are working on the problem who have an intuitive conviction that it is
possible somehow to describe the world in a different way and that all
of this uncertainty about the way things are can be removed. No one has
yet been successful.
</p>
<p class="p blue-text">
不确定性原理描述了在叙述⾃然界的任何尝试中所必然存在着的那种内在的模糊性或不明确性。我们对⾃然界的最准确描写必须⽤概率的观念。有些⼈不喜欢⽤这种⽅法来描写⾃然界。不知怎么地，他们总觉得，只要能说出⼀个粒⼦真正在做什么，他们就能同时知道它的速度和位置。在量⼦⼒学发展的初期，爱因斯坦曾为这个问题⼗分担忧。他常摇头说：“啊！上帝肯定不是⽤掷骰⼦来决定电⼦应如何运动的！”他为这个问题担忧了好长时间，或许他从来也没有使他⾃⼰真正相信过这个事实，即：这是⼈们对⾃然界所能作出的最好描述。现在仍然有⼀两位物理学家在研究这问题，他们从直觉上深信，可以通过某种⽅式⽤另⼀种⽅法来描写这个世界，并且可以把有关事物⾏为的所有这种不确定性都消除掉。然⽽到现在没有⼀个是成功的。
</p>
</div>
<div id="Ch6-S5-p6" class="para">
<p class="p">The necessary uncertainty in our specification of the position of a
particle becomes most important when we wish to describe the structure
of atoms. In the hydrogen atom, which has a nucleus of one proton with
one electron outside of the nucleus, the uncertainty in the position of
the electron is as large as the atom itself! We cannot, therefore,
properly speak of the electron moving in some “orbit” around the
proton. The most we can say is that there is a certain
<em class="emph">chance</em>&nbsp;$p(r)\,\Delta V$, of observing the electron in an element
of volume&nbsp;$\Delta V$ at the distance&nbsp;$r$ from the proton. The
probability density&nbsp;$p(r)$ is given by quantum mechanics. For an undisturbed hydrogen atom $p(r)=Ae^{-2r/a}$. The
number&nbsp;$a$ is the “typical” radius, where the function is decreasing
rapidly. Since there is a small probability of finding the electron at
distances from the nucleus much greater than&nbsp;$a$, we may think of&nbsp;$a$ as
“the radius of the atom,” about $10^{-10}$&nbsp;meter.
</p>
<p class="p blue-text">
当我们希望描写原⼦结构时，确定⼀个粒⼦的位置所必然要出现的不确定性就变得极为重要。在氢原⼦中有⼀个由单个质⼦组成的核，核的外⾯有⼀个电⼦，⽽这个电⼦的位置的不确定性就同原⼦本⾝⼀样⼤！因此我们不能严格地说，电⼦在某⼀“轨道”上绕质⼦运动。最多我们可以说，在⼀个离质⼦距离为 $r$ 的体积元 $\Delta V$ 内有⼀定的机会 $p(r)\,\Delta V$ 观察到这个电⼦，概率密度 $p(r)$ 由量⼦⼒学来确定。对⼀个未受扰动的氢原⼦来说，$p(r)=Ae^{-2r/a}$，这是⼀个如图 6-8 所示的那种钟形函数。数 $a$ 是“典型”的半径，函数由这⾥开始减⼩很快。既然在离原⼦核距离远⼤于 $a$ 的地⽅找到电⼦的概率很⼩，我们可以把 $a$ 设想为“原⼦的半径”，⼤约等于 $10^{-10}$ m。
</p>
</div>
<div id="Ch6-F11" class="figure">
<img src="./img/I_06/f06-11_small.jpg"><div class="caption">
<span class="tag">图 6–11.</span>使氢原子形象化的一种方法。这里云的密度（洁白度）表示能观察到的电子的概率密度
</div>
</div>
<div id="Ch6-S5-p7" class="para">
<p class="p">We can form an image of the hydrogen atom by imagining a “cloud” whose
density is proportional to the probability density for observing the
electron. A sample of such a cloud is shown in Fig.&nbsp;<a href="#Ch6-F11" class="">6–11</a>.
Thus our best “picture” of a hydrogen atom is a nucleus surrounded by
an “electron cloud” (although we <em class="emph">really</em>
mean a “probability cloud”). The electron is there somewhere, but
nature permits us to know only the <i>chance</i> of finding it at any
particular place.</p>
<p class="p blue-text">
如果想象有这样⼀团“云”，它的密度正⽐于我们能观察到的电⼦的概率密度，那么我们就能形成氢原⼦的图像。这样⼀团云的⼀个实例如图 6-11 所示。所以我们对氢原⼦的最好“写照”便是⼀团“电⼦云”（虽然我们实际上指的是“概率云”）围绕着⼀个核。电⼦就处在云中某⼀地⽅，但⾃然界只允许我们知道在任何⼀个特定位置上能找到它的机会是多少。
</p>
</div>
<div id="Ch6-S5-p8" class="para">
<p class="p">In its efforts to learn as much as possible about nature, modern physics
has found that certain things can never be “known” with certainty.
Much of our knowledge must always remain uncertain. The <em class="emph">most</em> we
can know is in terms of probabilities.</p>
<p class="p blue-text">
在尽可能多地了解⾃然界的努⼒中，现代物理学曾发现，有些事情永远不可能确切地“知道”。我们的许多知识必然总是不确定的。⽽⽤概率来表述时，我们所能获得的知识则最多。
</p>
</div>
</div>

-----
[^1]: 在前三轮游戏之后，实际上是这样进⾏实验的，即把放在⼀只盒⼦中的 $30$ 个分币剧烈摇动，然后数⼀下出现正⾯的数⽬。


  <a href="#footnote_source_1">↩</a>
</li>
<li class="footnote">
  <a id="footnote_2"></a>
  麦克斯韦的表示式是 $p(v)=Cv^2e^{-av^2}$，其中 $a$ 是⼀个与温度有关的常数，⽽ $C$ 应选定得使总的概率等于 $1$。
  <a href="#footnote_source_2">↩</a>
</li>
</ol>
</div>
<footer>
   <a href="http:/localhost:8000/I_copyright.html"><span>Copyright © 1963, 2006, 2013</span>
   <span>by the California Institute of Technology,</span>
   <span>Michael A. Gottlieb and Rudolf Pfeiffer</span></a>
</footer>
</div>
</div>

</div>   

Since P(k,n)P(k,n) is the fraction of games which we expect to yield kk heads, then in 100100 games we should expect to find kk heads 100⋅P(k,n)100⋅P(k,n) times. The dashed curve in Fig. [6–2](https://www.feynmanlectures.caltech.edu/I_06.html#Ch6-F2) passes through the points computed from 100⋅P(k,30)100⋅P(k,30). We see that we _expect_ to obtain a score of 1515 heads in 1414 or 1515 games, whereas this score was observed in 1313 games. We _expect_ a score of 1616 in 1313 or 1414 games, but we obtained that score in 1515 games. Such fluctuations are “part of the game.”

The method we have just used can be applied to the most general situation in which there are only two possible outcomes of a single observation. Let us designate the two outcomes by WW (for “win”) and LL (for “lose”). In the general case, the probability of WW or LL in a single event need not be equal. Let pp be the probability of obtaining the result WW. Then qq, the probability of LL, is necessarily (1−p)(1−p). In a set of nn trials, the probability P(k,n)P(k,n) that WW will be obtained kk times is

P(k,n)=(nk)pkqn−k.(6.6)(6.6)P(k,n)=(nk)pkqn−k.

This probability function is called the _Bernoulli_ or, also, the _binomial_ probability.

### 6–3The random walk

There is another interesting problem in which the idea of probability is required. It is the problem of the “random walk.” In its simplest version, we imagine a “game” in which a “player” starts at the point x=0x=0 and at each “move” is required to take a step _either_ forward (toward +x+x) _or_ backward (toward −x−x). The choice is to be made _randomly_, determined, for example, by the toss of a coin. How shall we describe the resulting motion? In its general form the problem is related to the motion of atoms (or other particles) in a gas—called Brownian motion—and also to the combination of errors in measurements. You will see that the random-walk problem is closely related to the coin-tossing problem we have already discussed.

First, let us look at a few examples of a random walk. We may characterize the walker’s progress by the net distance DNDN traveled in NN steps. We show in the graph of Fig. [6–5](https://www.feynmanlectures.caltech.edu/I_06.html#Ch6-F5) three examples of the path of a random walker. (We have used for the random sequence of choices the results of the coin tosses shown in Fig. [6–1](https://www.feynmanlectures.caltech.edu/I_06.html#Ch6-F1).)

![](https://www.feynmanlectures.caltech.edu/img/FLP_I/f06-05/f06-05_tc_big.svgz)

Fig. 6–5.The progress made in a random walk. The horizontal coordinate NN is the total number of steps taken; the vertical coordinate DNDN is the net distance moved from the starting position.

What can we say about such a motion? We might first ask: “How far does he get on the average?” We must _expect_ that his average progress will be zero, since he is equally likely to go either forward or backward. But we have the feeling that as NN increases, he is more likely to have strayed farther from the starting point. We might, therefore, ask what is his average distance travelled in _absolute value_, that is, what is the average of |D||D|. It is, however, more convenient to deal with another measure of “progress,” the square of the distance: D2D2 is positive for either positive or negative motion, and is therefore a reasonable _measure_ of such random wandering.

We can show that the expected value of D2NDN2 is just NN, the number of steps taken. By “expected value” we mean the probable value (our best guess), which we can think of as the _expected_ average behavior in _many repeated_ sequences. We represent such an expected value by ⟨D2N⟩⟨DN2⟩, and may refer to it also as the “mean square distance.” After one step, D2D2 is always +1+1, so we have certainly ⟨D21⟩=1⟨D12⟩=1. (All distances will be measured in terms of a unit of one step. We shall not continue to write the units of distance.)

The expected value of D2NDN2 for N>1N>1 can be obtained from DN−1DN−1. If, after (N−1)(N−1) steps, we have DN−1DN−1, then after NN steps we have DN=DN−1+1DN=DN−1+1 _or_ DN=DN−1−1DN=DN−1−1. For the squares,

D2N=⎧⎩⎨⎪⎪⎪⎪⎪⎪⎪⎪D2N−1+2DN−1+1,orD2N−1−2DN−1+1.(6.7)(6.7)DN2={DN−12+2DN−1+1,orDN−12−2DN−1+1.

In a number of independent sequences, we expect to obtain each value one-half of the time, so our average expectation is just the average of the two possible values. The expected value of D2NDN2 is then D2N−1+1DN−12+1. _In general_, we should _expect_ for D2N−1DN−12 its “expected value” ⟨D2N−1⟩⟨DN−12⟩ (by definition!). So

⟨D2N⟩=⟨D2N−1⟩+1.(6.8)(6.8)⟨DN2⟩=⟨DN−12⟩+1.

We have already shown that ⟨D21⟩=1⟨D12⟩=1; it follows then that

⟨D2N⟩=N,(6.9)(6.9)⟨DN2⟩=N,

a particularly simple result!

If we wish a number like a distance, rather than a distance squared, to represent the “progress made away from the origin” in a random walk, we can use the “root-mean-square distance” DrmsDrms:

Drms=⟨D2⟩−−−−√=N−−√.(6.10)(6.10)Drms=⟨D2⟩=N.

We have pointed out that the random walk is closely similar in its mathematics to the coin-tossing game we considered at the beginning of the chapter. If we imagine the direction of each step to be in correspondence with the appearance of heads or tails in a coin toss, then DD is just NH−NTNH−NT, the difference in the number of heads and tails. Since NH+NT=NNH+NT=N, the total number of steps (and tosses), we have D=2NH−ND=2NH−N. We have derived earlier an expression for the expected distribution of NHNH (also called kk) and obtained the result of Eq. ([6.5](https://www.feynmanlectures.caltech.edu/I_06.html#mjx-eqn-EqI65)). Since NN is just a constant, we have the corresponding distribution for DD. (Since for every head more than N/2N/2 there is a tail “missing,” we have the factor of 22 between NHNH and DD.) The graph of Fig. [6–2](https://www.feynmanlectures.caltech.edu/I_06.html#Ch6-F2) represents the distribution of distances we might get in 3030 random steps (where k=15k=15 is to be read D=0D=0; k=16k=16, D=2D=2; etc.).

The variation of NHNH from its expected value N/2N/2 is

NH−N2=D2.(6.11)(6.11)NH−N2=D2.

The rms deviation is

(NH−N2)rms=12N−−√.(6.12)(6.12)(NH−N2)rms=12N.

According to our result for DrmsDrms, we expect that the “typical” distance in 3030 steps ought to be 30−−√≈5.530≈5.5, or a typical kk should be about 5.5/2=2.755.5/2=2.75 units from 1515. We see that the “width” of the curve in Fig. [6–2](https://www.feynmanlectures.caltech.edu/I_06.html#Ch6-F2), measured from the center, is just about 33 units, in agreement with this result.

We are now in a position to consider a question we have avoided until now. How shall we tell whether a coin is “honest” or “loaded”? We can give now at least a partial answer. For an honest coin, we expect the fraction of the times heads appears to be 0.50.5, that is,

⟨NH⟩N=0.5.(6.13)(6.13)⟨NH⟩N=0.5.

We _also_ expect an actual NHNH to deviate from N/2N/2 by about N−−√/2N/2, or the _fraction_ to deviate by

1NN−−√2=12N−−√.1NN2=12N.

The larger NN is, the closer we _expect_ the fraction NH/NNH/N to be to one-half.

![](https://www.feynmanlectures.caltech.edu/img/FLP_I/f06-06/f06-06_tc_big.svgz)

Fig. 6–6.The fraction of the tosses that gave heads in a particular sequence of NN tosses of a penny.

In Fig. [6–6](https://www.feynmanlectures.caltech.edu/I_06.html#Ch6-F6) we have plotted the fraction NH/NNH/N for the coin tosses reported earlier in this chapter. We see the tendency for the fraction of heads to approach 0.50.5 for large NN. Unfortunately, for any given run or combination of runs there is no _guarantee_ that the observed deviation will be even _near_ the _expected_ deviation. There is always the finite chance that a large fluctuation—a long string of heads or tails—will give an arbitrarily large deviation. All we can say is that _if_ the deviation is near the expected 1/2N−−√1/2N (say within a factor of 22 or 33), we have no reason to suspect the honesty of the coin. If it is much larger, we may be suspicious, but cannot prove, that the coin is loaded (or that the tosser is clever!).

We have also not considered how we should treat the case of a “coin” or some similar “chancy” object (say a stone that always lands in either of two positions) that we have good reason to believe _should_ have a different probability for heads and tails. We have defined P(H)=⟨NH⟩/NP(H)=⟨NH⟩/N. How shall we know what to _expect_ for NHNH? In some cases, the best we can do is to observe the number of heads obtained in large numbers of tosses. For want of anything better, we must set ⟨NH⟩=NH(observed)⟨NH⟩=NH(observed). (How could we expect anything else?) We must understand, however, that in such a case a different experiment, or a different observer, might conclude that P(H)P(H) was different. We would _expect_, however, that the various answers should agree within the deviation 1/2N−−√1/2N [if P(H)P(H) is near one-half]. An experimental physicist usually says that an “experimentally determined” probability has an “error,” and writes

P(H)=NHN±12N−−√.(6.14)(6.14)P(H)=NHN±12N.

There is an implication in such an expression that there _is_ a “true” or “correct” probability which _could_ be computed if we knew enough, and that the observation may be in “error” due to a fluctuation. There is, however, no way to make such thinking logically consistent. It is probably better to realize that the probability concept is in a sense subjective, that it is always based on uncertain knowledge, and that its quantitative evaluation is subject to change as we obtain more information.

### 6–4A probability distribution

Let us return now to the random walk and consider a modification of it. Suppose that in addition to a random choice of the _direction_ (++ or −−) of each step, the _length_ of each step also varied in some unpredictable way, the only condition being that _on the average_ the step length was one unit. This case is more representative of something like the thermal motion of a molecule in a gas. If we call the length of a step SS, then SS may have any value at all, but most often will be “near” 11. To be specific, we shall let ⟨S2⟩=1⟨S2⟩=1 or, equivalently, Srms=1Srms=1. Our derivation for ⟨D2⟩⟨D2⟩ would proceed as before except that Eq. ([6.8](https://www.feynmanlectures.caltech.edu/I_06.html#mjx-eqn-EqI68)) would be changed now to read

⟨D2N⟩=⟨D2N−1⟩+⟨S2⟩=⟨D2N−1⟩+1.(6.15)(6.15)⟨DN2⟩=⟨DN−12⟩+⟨S2⟩=⟨DN−12⟩+1.

We have, as before, that

⟨D2N⟩=N.(6.16)(6.16)⟨DN2⟩=N.

What would we expect now for the distribution of distances DD? What is, for example, the probability that D=0D=0 after 3030 steps? The answer is zero! The probability is zero that DD will be _any particular_ value, since there is no chance at all that the sum of the backward steps (of varying lengths) would exactly equal the sum of forward steps. We cannot plot a graph like that of Fig. [6–2](https://www.feynmanlectures.caltech.edu/I_06.html#Ch6-F2).

We can, however, obtain a representation similar to that of Fig. [6–2](https://www.feynmanlectures.caltech.edu/I_06.html#Ch6-F2), if we ask, not what is the probability of obtaining DD exactly equal to 00, 11, or 22, but instead what is the probability of obtaining DD _near_ 00, 11, or 22. Let us define P(x,Δx)P(x,Δx) as the probability that DD will lie in the interval ΔxΔx located at xx (say from xx to x+Δxx+Δx). We expect that for small ΔxΔx the chance of DD landing in the interval is proportional to ΔxΔx, the width of the interval. So we can write

P(x,Δx)=p(x)Δx.(6.17)(6.17)P(x,Δx)=p(x)Δx.

The function p(x)p(x) is called the _probability density_.

The form of p(x)p(x) will depend on NN, the number of steps taken, and also on the distribution of individual step lengths. We cannot demonstrate the proofs here, but for large NN, p(x)p(x) is the _same_ for all reasonable distributions in individual step lengths, and depends only on NN. We plot p(x)p(x) for three values of NN in Fig. [6–7](https://www.feynmanlectures.caltech.edu/I_06.html#Ch6-F7). You will notice that the “half-widths” (typical spread from x=0x=0) of these curves is N−−√N, as we have shown it should be.

![](https://www.feynmanlectures.caltech.edu/img/FLP_I/f06-07/f06-07_tc_big.svgz)

Fig. 6–7.The probability density for ending up at the distance DD from the starting place in a random walk of NN steps. (DD is measured in units of the rms step length.)

You may notice also that the value of p(x)p(x) near zero is inversely proportional to N−−√N. This comes about because the curves are all of a similar shape and their areas under the curves must all be equal. Since p(x)Δxp(x)Δx is the probability of finding DD in ΔxΔx when ΔxΔx is small, we can determine the chance of finding DD _somewhere_ inside an arbitrary interval from x1x1 to x2x2, by cutting the interval in a number of small increments ΔxΔx and evaluating the sum of the terms p(x)Δxp(x)Δx for each increment. The probability that DD lands somewhere between x1x1 and x2x2, which we may write P(x1<D<x2)P(x1<D<x2), is equal to the shaded area in Fig. [6–8](https://www.feynmanlectures.caltech.edu/I_06.html#Ch6-F8). The smaller we take the increments ΔxΔx, the more correct is our result. We can write, therefore,

P(x1<D<x2)=∑p(x)Δx=∫x2x1p(x)dx.(6.18)(6.18)P(x1<D<x2)=∑p(x)Δx=∫x1x2p(x)dx.

![](https://www.feynmanlectures.caltech.edu/img/FLP_I/f06-08/f06-08_tc_big.svgz)

Fig. 6–8.The probability that the distance DD traveled in a random walk is between x1x1 and x2x2 is the area under the curve of p(x)p(x) from x1x1 to x2x2.

The area under the whole curve is the probability that DD lands somewhere (that is, has _some_ value between x=−∞x=−∞ and x=+∞x=+∞). That probability is surely 11. We must have that

∫+∞−∞p(x)dx=1.(6.19)(6.19)∫−∞+∞p(x)dx=1.

Since the curves in Fig. [6–7](https://www.feynmanlectures.caltech.edu/I_06.html#Ch6-F7) get wider in proportion to N−−√N, their heights must be proportional to 1/N−−√1/N to maintain the total area equal to 11.

The probability density function we have been describing is one that is encountered most commonly. It is known as the _normal_ or _Gaussian_ probability density. It has the mathematical form

p(x)=1σ2π−−√e−x2/2σ2,(6.20)(6.20)p(x)=1σ2πe−x2/2σ2,

where σσ is called the _standard deviation_ and is given, in our case, by σ=N−−√σ=N or, if the rms step size is different from 11, by σ=N−−√Srmsσ=NSrms.

We remarked earlier that the motion of a molecule, or of any particle, in a gas is like a random walk. Suppose we open a bottle of an organic compound and let some of its vapor escape into the air. If there are air currents, so that the air is circulating, the currents will also carry the vapor with them. But even in _perfectly still air_, the vapor will gradually spread out—will diffuse—until it has penetrated throughout the room. We might detect it by its color or odor. The individual molecules of the organic vapor spread out in still air because of the molecular motions caused by collisions with other molecules. If we know the average “step” size, and the number of steps taken per second, we can find the probability that one, or several, molecules will be found at some distance from their starting point after any particular passage of time. As time passes, more steps are taken and the gas spreads out as in the successive curves of Fig. [6–7](https://www.feynmanlectures.caltech.edu/I_06.html#Ch6-F7). In a later chapter, we shall find out how the step sizes and step frequencies are related to the temperature and pressure of a gas.

Earlier, we said that the pressure of a gas is due to the molecules bouncing against the walls of the container. When we come later to make a more quantitative description, we will wish to know how fast the molecules are going when they bounce, since the impact they make will depend on that speed. We cannot, however, speak of _the_ speed of the molecules. It is necessary to use a probability description. A molecule may have any speed, but some speeds are more likely than others. We describe what is going on by saying that the probability that any particular molecule will have a speed between vv and v+Δvv+Δv is p(v)Δvp(v)Δv, where p(v)p(v), a probability density, is a given function of the speed vv. We shall see later how Maxwell, using common sense and the ideas of probability, was able to find a mathematical expression for p(v)p(v). The form[2](https://www.feynmanlectures.caltech.edu/I_06.html#footnote_2) of the function p(v)p(v) is shown in Fig. [6–9](https://www.feynmanlectures.caltech.edu/I_06.html#Ch6-F9). Velocities may have any value, but are most likely to be near the most probable value vpvp.

![](https://www.feynmanlectures.caltech.edu/img/FLP_I/f06-09/f06-09_tc_big.svgz)

Fig. 6–9.The distribution of velocities of the molecules in a gas.

We often think of the curve of Fig. [6–9](https://www.feynmanlectures.caltech.edu/I_06.html#Ch6-F9) in a somewhat different way. If we consider the molecules in a typical container (with a volume of, say, one liter), then there are a very large number NN of molecules present (N≈1022N≈1022). Since p(v)Δvp(v)Δv is the probability that _one_ molecule will have its velocity in ΔvΔv, by our definition of probability we mean that the _expected_ number ⟨ΔN⟩⟨ΔN⟩ to be found with a velocity in the interval ΔvΔv is given by

⟨ΔN⟩=Np(v)Δv.(6.21)(6.21)⟨ΔN⟩=Np(v)Δv.

We call Np(v)Np(v) the “distribution in velocity.” The area under the curve between two velocities v1v1 and v2v2, for example the shaded area in Fig. [6–9](https://www.feynmanlectures.caltech.edu/I_06.html#Ch6-F9), represents [for the curve Np(v)Np(v)] the expected number of molecules with velocities between v1v1 and v2v2. Since with a gas we are usually dealing with large numbers of molecules, we expect the deviations from the expected numbers to be small (like 1/N−−√1/N), so we often neglect to say the “expected” number, and say instead: “The number of molecules with velocities between v1v1 and v2v2 _is_ the area under the curve.” We should remember, however, that such statements are always about _probable_ numbers.

### 6–5The uncertainty principle

The ideas of probability are certainly useful in describing the behavior of the 10221022 or so molecules in a sample of a gas, for it is clearly impractical even to attempt to write down the position or velocity of each molecule. When probability was first applied to such problems, it was considered to be a _convenience_—a way of dealing with very complex situations. We now believe that the ideas of probability are _essential_ to a description of atomic happenings. According to quantum mechanics, the mathematical theory of particles, there is always some uncertainty in the _specification_ of positions and velocities. We can, at best, say that there is a certain probability that any particle will have a position near some coordinate xx.

We can give a probability density p1(x)p1(x), such that p1(x)Δxp1(x)Δx is the probability that the particle will be found between xx and x+Δxx+Δx. If the particle is reasonably well localized, say near x0x0, the function p1(x)p1(x) might be given by the graph of Fig. [6–10](https://www.feynmanlectures.caltech.edu/I_06.html#Ch6-F10)(a). Similarly, we must specify the velocity of the particle by means of a probability density p2(v)p2(v), with p2(v)Δvp2(v)Δv the probability that the velocity will be found between vv and v+Δvv+Δv.

![](https://www.feynmanlectures.caltech.edu/img/FLP_I/f06-10/f06-10_tc_big_a.svgz)![](https://www.feynmanlectures.caltech.edu/img/FLP_I/f06-10/f06-10_tc_big_b.svgz)

Fig. 6–10.Probability densities for observation of the position and velocity of a particle.

It is one of the fundamental results of quantum mechanics that the two functions p1(x)p1(x) and p2(v)p2(v) cannot be chosen independently and, in particular, cannot both be made arbitrarily narrow. If we call the typical “width” of the p1(x)p1(x) curve [Δx][Δx], and that of the p2(v)p2(v) curve [Δv][Δv] (as shown in the figure), nature demands that the _product_ of the two widths be at least as big as the number ℏ/2mℏ/2m, where mm is the mass of the particle. We may write this basic relationship as

[Δx]⋅[Δv]≥ℏ/2m.(6.22)(6.22)[Δx]⋅[Δv]≥ℏ/2m.

This equation is a statement of the _Heisenberg uncertainty principle_ that we mentioned earlier.

Since the right-hand side of Eq. ([6.22](https://www.feynmanlectures.caltech.edu/I_06.html#mjx-eqn-EqI622)) is a constant, this equation says that if we try to “pin down” a particle by forcing it to be at a particular place, it ends up by having a high speed. Or if we try to force it to go very slowly, or at a precise velocity, it “spreads out” so that we do not know very well just where it is. Particles behave in a funny way!

The uncertainty principle describes an inherent fuzziness that must exist in any attempt to describe nature. Our most precise description of nature _must_ be in terms of _probabilities_. There are some people who do not like this way of describing nature. They feel somehow that if they could only tell what is _really_ going on with a particle, they could know its speed and position simultaneously. In the early days of the development of quantum mechanics, Einstein was quite worried about this problem. He used to shake his head and say, “But, surely God does not throw dice in determining how electrons should go!” He worried about that problem for a long time and he probably never really reconciled himself to the fact that this is the best description of nature that one can give. There are still one or two physicists who are working on the problem who have an intuitive conviction that it is possible somehow to describe the world in a different way and that all of this uncertainty about the way things are can be removed. No one has yet been successful.

The necessary uncertainty in our specification of the position of a particle becomes most important when we wish to describe the structure of atoms. In the hydrogen atom, which has a nucleus of one proton with one electron outside of the nucleus, the uncertainty in the position of the electron is as large as the atom itself! We cannot, therefore, properly speak of the electron moving in some “orbit” around the proton. The most we can say is that there is a certain _chance_ p(r)ΔVp(r)ΔV, of observing the electron in an element of volume ΔVΔV at the distance rr from the proton. The probability density p(r)p(r) is given by quantum mechanics. For an undisturbed hydrogen atom p(r)=Ae−2r/ap(r)=Ae−2r/a. The number aa is the “typical” radius, where the function is decreasing rapidly. Since there is a small probability of finding the electron at distances from the nucleus much greater than aa, we may think of aa as “the radius of the atom,” about 10−1010−10 meter.

![](https://www.feynmanlectures.caltech.edu/img/FLP_I/f06-11/f06-11_small.jpg)

Fig. 6–11.A way of visualizing a hydrogen atom. The density (whiteness) of the cloud represents the probability density for observing the electron.

We can form an image of the hydrogen atom by imagining a “cloud” whose density is proportional to the probability density for observing the electron. A sample of such a cloud is shown in Fig. [6–11](https://www.feynmanlectures.caltech.edu/I_06.html#Ch6-F11). Thus our best “picture” of a hydrogen atom is a nucleus surrounded by an “electron cloud” (although we _really_ mean a “probability cloud”). The electron is there somewhere, but nature permits us to know only the _chance_ of finding it at any particular place.

In its efforts to learn as much as possible about nature, modern physics has found that certain things can never be “known” with certainty. Much of our knowledge must always remain uncertain. The _most_ we can know is in terms of probabilities.

1. After the first three games, the experiment was actually done by shaking 3030 pennies violently in a box and then counting the number of heads that showed. [↩](https://www.feynmanlectures.caltech.edu/I_06.html#footnote_source_1)
2. Maxwell’s expression is p(v)=Cv2e−av2p(v)=Cv2e−av2, where aa is a constant related to the temperature and CC is chosen so that the total probability is one. [↩](https://www.feynmanlectures.caltech.edu/I_06.html#footnote_source_2)

[Copyright © 1963, 2006, 2013 by the California Institute of Technology, Michael A. Gottlieb and Rudolf Pfeiffer](https://www.feynmanlectures.caltech.edu/I_copyright.html)

